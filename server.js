// server.js â€” gpt-server-060kc (Render)ìš© [ESM + RAG ì—°ë™/ë‚´êµ¬ì„± ê°•í™”ë³¸]
import express from "express";
import cors from "cors";
import dotenv from "dotenv";
import OpenAI from "openai";

dotenv.config();

const FALLBACK_LINES = [
  "ê·¸ ì‚¬í•­ì€ íšŒì‚¬ ë‚´ë¶€ ìë£Œì—ì„œ ê²€ìƒ‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
  "íšŒì‚¬ ë‚´ë¶€ ìë£ŒëŠ” ë³´ì™„í•˜ê² ìŠµë‹ˆë‹¤.",
  "ì˜¤ëŠ˜ì€ ìš°ì„  ë‹´ë‹¹ìì—ê²Œ ë¬¸ì˜ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.",
  "ğŸ“ 070-8231-8295ë¡œ ë°”ë¡œ ì „í™”í•˜ê¸° / í‰ì¼ 09:00â€“18:00"
];
const FALLBACK_MSG = FALLBACK_LINES.join("\n");

console.log("ğŸš€ 060KC gpt-server boot :: with /company-chat (RAG) & /chat");

const app = express();

// ===== í•„ìˆ˜ í™˜ê²½ =====
const PORT = Number(process.env.PORT);
if (!PORT) {
  console.error("âŒ PORT env missing");
  process.exit(1);
}

// RAG ì„œë²„ ì—”ë“œí¬ì¸íŠ¸ (í•„ìˆ˜: í™˜ê²½ë³€ìˆ˜ ê¶Œì¥)
const RAG_ENDPOINT =
  process.env.RAG_ENDPOINT || "https://zero60kc-rag.onrender.com/ask";

// (ì„ íƒ) ë³´ì¡°íŒì •ìš© ì„ê³„ê°’ â€” RAG ì„œë²„ì˜ thresholdì™€ ë§ì¶”ë©´ ì¢‹ìŒ
const RAG_THRESHOLD = Number(process.env.RAG_THRESHOLD || 0.35);

// ===== ê³µí†µ ë¯¸ë“¤ì›¨ì–´ =====
app.use(
  cors({
    origin: [
      "https://www.060kc.com",
      "https://060kc.com",
      "http://localhost:8080",
      "http://127.0.0.1:8080"
    ],
    methods: ["POST", "GET", "OPTIONS"],
    allowedHeaders: ["Content-Type", "Authorization"]
  })
);
app.use(express.json({ limit: "2mb" }));

// í—¬ìŠ¤ì²´í¬
app.get("/health", (_req, res) => res.json({ ok: true, ts: Date.now() }));

// ì‚¬ëŒì´ìŒ í…œí”Œë¦¿
function handoffTemplate() {
  return `ì•ˆë…•í•˜ì„¸ìš”! ì €í¬ 060 ì¼€ì´ì”¨ AI ìƒë‹´ì›ì…ë‹ˆë‹¤.
ì •í™•í•œ ì•ˆë‚´ë¥¼ ìœ„í•´ ë‹´ë‹¹ì ì—°ê²°ì„ ë°”ë¡œ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì§€ê¸ˆ ì—°ê²°í•´ ë“œë¦´ê¹Œìš”?
[ë¬¸ì˜: 070-8231-8295, í‰ì¼ 09:00â€“18:00]`;
}

// â€œìë£Œì— ì—†ìŒâ€ ì™„ì „ì¼ì¹˜ íŒë³„(ê³µë°±/ë§ˆì¹¨í‘œ ì œê±° í›„ ë¹„êµ)
function isExactNoData(text = "") {
  const compact = String(text).replace(/[\s.]/g, "");
  const candidates = [
    "ìë£Œì—ì—†ìŠµë‹ˆë‹¤ê³ ê°ì„¼í„°ë¡œë¬¸ì˜í•´ì£¼ì„¸ìš”",
    "ìë£Œì—ì—†ìŠµë‹ˆë‹¤ê³ ê°ì„¼í„°ë¡œë¬¸ì˜í•´ì£¼ì„¸ìš”",
    "ìë£Œì—ì—†ìŒ"
  ];
  return candidates.includes(compact);
}

// ----- RAG í˜¸ì¶œ ìœ í‹¸ -----
async function fetchRag(question) {
  try {
    const r = await fetch(RAG_ENDPOINT, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      // rewrite íŒŒë¼ë¯¸í„°ëŠ” RAG ì„œë²„ì—ì„œ ë¬´ì‹œí•´ë„ ë¬´ë°©
      body: JSON.stringify({ question, rewrite: true })
    });

    const data = await r.json().catch(() => ({}));
    const hits = Array.isArray(data.hits) ? data.hits : [];
    const bestScore =
      typeof data.bestScore === "number" ? data.bestScore : (hits[0]?.score ?? 0);

    // ë³´ì¡° íŒì •: RAG ì„œë²„ì˜ foundê°€ trueì´ê±°ë‚˜, ì ìˆ˜ê°€ ì„ê³„ê°’ ì´ìƒ
    const found = data.found === true || (hits.length > 0 && bestScore >= RAG_THRESHOLD);

    // ìƒìœ„ íˆíŠ¸ í…ìŠ¤íŠ¸ í•©ì³ì„œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    const top = hits.slice(0, 5);
    const context = top
      .map(h => (h.text || `${h.question ?? ""} ${h.answer ?? ""}`).trim())
      .filter(Boolean)
      .join("\n\n");

    return {
      found,
      bestScore,
      answer: (data.answer || "").trim(),
      context
    };
  } catch (e) {
    console.error("[RAG fetch error]", e?.message || e);
    return { found: false, bestScore: 0, answer: "", context: "" };
  }
}

// ----- ë””ë²„ê·¸: gpt-server â†’ RAG ì§í†µ í•‘ -----
app.get("/debug/rag-ping", async (_req, res) => {
  try {
    const r = await fetch(RAG_ENDPOINT, {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ question: "ì„¤ì¹˜ë¹„ê°€ ìˆë‚˜ìš”?" })
    });
    const text = await r.text();
    res.status(r.status).type("text/plain").send(text);
  } catch (e) {
    res.status(500).type("text/plain").send(String(e?.message || e));
  }
});

// ----- íšŒì‚¬ ì „ìš© ë¼ìš°íŠ¸ (/company-chat) -----
// 1) ì§ˆë¬¸ íŒŒì‹±(ì—¬ëŸ¬ í‚¤ í—ˆìš©) â†’ 2) RAG ê²€ìƒ‰ â†’ 3) ì§ì ‘ë‹µ or ì»¨í…ìŠ¤íŠ¸ ìš”ì•½ â†’ 4) ì‹¤íŒ¨ ì‹œ ì‚¬ëŒì´ìŒ
app.post("/company-chat", async (req, res) => {
  try {
    // í”„ëŸ°íŠ¸ê°€ ì–´ë–¤ í‚¤ë¡œ ë³´ë‚´ë„ ì½íˆë„ë¡ ë°©ì–´
    const question = (
      req.body?.question ??
      req.body?.q ??
      req.body?.message ??
      req.body?.text ??
      req.body?.prompt ??
      ""
    ).toString().trim();

    if (!question) {
      console.warn("[company-chat] empty question body:", req.body);
      return res.json({ reply: FALLBACK_MSG, needs_handoff: true });
    }

    // 1) RAG í˜¸ì¶œ
    const rag = await fetchRag(question);
    console.log("[RAG]", { found: rag.found, bestScore: rag.bestScore, ctxLen: rag.context?.length || 0 });

    // 2) RAGê°€ ì§ì ‘ ì •ë‹µì„ ì£¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ë‹¨, â€œìë£Œì— ì—†ìŒâ€ ì™„ì „ì¼ì¹˜ ì œì™¸)
    if (rag.answer && !isExactNoData(rag.answer) && rag.found) {
      return res.json({ reply: rag.answer, needs_handoff: false });
    }


    // 2-1) ë‚´ë¶€ ìë£Œ ë¯¸ë°œê²¬ â†’ ì¦‰ì‹œ í´ë°±
    const noData =
      !rag?.found ||
      isExactNoData(rag?.answer) ||
      (typeof rag?.bestScore === "number" && rag.bestScore < RAG_THRESHOLD);
    if (noData) {
      return res.json({ reply: FALLBACK_MSG, needs_handoff: true });
    }



    // 3) ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ OpenAIë¡œ ìš”ì•½ ì‹œë„ (found ì—¬ë¶€ì™€ ë¬´ê´€)
    if (rag.context) {
      const systemPrompt = `
ë‹¹ì‹ ì€ 060 ì¼€ì´ì”¨(060KC) íšŒì‚¬ ë¬¸ì„œ ì „ìš© ìƒë‹´ì›ì…ë‹ˆë‹¤.
ì•„ë˜ CONTEXT(íšŒì‚¬ ìë£Œ)ì— í¬í•¨ëœ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µí•©ë‹ˆë‹¤.
CONTEXTì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ì •í•˜ì§€ ë§ê³  ë‹¤ìŒ ë¬¸êµ¬ë¡œë§Œ ë‹µí•©ë‹ˆë‹¤:
"ìë£Œì— ì—†ìŠµë‹ˆë‹¤. ê³ ê°ì„¼í„°ë¡œ ë¬¸ì˜í•´ ì£¼ì„¸ìš”."
ë¬¸ì²´: ì •ì¤‘í•˜ê³  ê°„ê²°. ê³¼ì¥/ì¶”ì¸¡/ì¼ë°˜ì§€ì‹ ì‚¬ìš© ê¸ˆì§€.
`.trim();

      const userPrompt = `
[QUESTION]
${question}

[CONTEXT]
${rag.context}
`.trim();

      const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
      const r = await openai.chat.completions.create({
        model: process.env.CHAT_MODEL || "gpt-4o-mini",
        temperature: 0.0,
        messages: [
          { role: "system", content: systemPrompt },
          { role: "user", content: userPrompt }
        ]
      });

      const reply = (r.choices?.[0]?.message?.content || "").trim();
      console.log("[LLM reply]", reply);

      if (reply && !isExactNoData(reply)) {
        return res.json({ reply, needs_handoff: false });
      }
    }

    // 4) ëª¨ë‘ ì‹¤íŒ¨ ì‹œ ì‚¬ëŒì´ìŒ
    return res.json({ reply: FALLBACK_MSG, needs_handoff: true });
  } catch (e) {
    console.error("[/company-chat] error:", e?.message || e);
    return res.status(500).json({ reply: FALLBACK_MSG, needs_handoff: true });
  }
});

// (ì˜µì…˜) ë²”ìš© ì±„íŒ… ìœ ì§€
app.post("/chat", async (req, res) => {
  try {
    const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
    const userMessage = (req.body?.message || "").toString();
    const r = await openai.chat.completions.create({
      model: process.env.CHAT_MODEL || "gpt-4o-mini",
      temperature: 0.3,
      messages: [
        { role: "system", content: "You are a helpful assistant." },
        { role: "user", content: userMessage }
      ]
    });
    res.json({ reply: r.choices?.[0]?.message?.content ?? "" });
  } catch (e) {
    console.error("[/chat] error:", e?.message || e);
    res.status(500).json({ error: "GPT ì„œë²„ ì˜¤ë¥˜ ë°œìƒ" });
  }
});

app.listen(PORT, "0.0.0.0", () => {
  console.log(`âœ… company-chat ONLINE on 0.0.0.0:${PORT}`);
});
